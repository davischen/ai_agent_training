# -*- coding: utf-8 -*-
"""
Inference Engine for ATLAS Agent
Handles real-time inference requests
"""

import logging
import numpy as np
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime

# ä¿®æ”¹ï¼šæ”¹ç‚ºåŒä¸€å±¤ç›®éŒ„çš„å°å…¥
from core.model_manager import ModelManager

logger = logging.getLogger(__name__)

@dataclass
class InferenceRequest:
    """Request object for inference operations"""
    request_id: str
    text_data: List[str]
    model_name: str = "default"
    batch_size: int = 32
    return_embeddings: bool = False
    threshold: float = 0.5
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

class InferenceEngine:
    """Handles real-time inference requests"""
    
    def __init__(self, model_manager: ModelManager):
        self.model_manager = model_manager
        
    async def process_inference(self, request: InferenceRequest) -> Dict[str, Any]:
        """Process inference request"""
        try:
            model = self.model_manager.load_model(request.model_name)
            
            # Generate embeddings
            embeddings = model.encode(
                request.text_data,
                batch_size=request.batch_size,
                convert_to_numpy=True,
                show_progress_bar=False
            )
            
            # Perform classification (simplified binary classification)
            predictions = []
            confidences = []
            
            for embedding in embeddings:
                # Simplified classification logic - replace with your actual classification method
                confidence = self._calculate_phishing_probability(embedding)
                prediction = 1 if confidence > request.threshold else 0
                predictions.append(prediction)
                confidences.append(confidence)
            
            result = {
                'predictions': predictions,
                'confidences': confidences,
                'text_data': request.text_data,
                'threshold_used': request.threshold,
                'model_used': request.model_name
            }
            
            if request.return_embeddings:
                result['embeddings'] = embeddings.tolist()
            
            return result
            
        except Exception as e:
            logger.error(f"Inference failed: {e}")
            raise
    
    def _calculate_phishing_probability(self, embedding: np.ndarray) -> float:
        """
        Calculate phishing probability from embedding
        
        æ³¨æ„ï¼šé€™æ˜¯ç°¡åŒ–çš„å¯¦ç¾ã€‚åœ¨å¯¦éš›ä½¿ç”¨ä¸­ï¼Œæ‚¨æ‡‰è©²ï¼š
        1. ä½¿ç”¨è¨“ç·´å¥½çš„åˆ†é¡å™¨ï¼ˆå¦‚ SVMã€RandomForest ç­‰ï¼‰
        2. æˆ–è€…ä½¿ç”¨èˆ‡è¨“ç·´æ™‚ç›¸åŒçš„ç›¸ä¼¼åº¦è¨ˆç®—æ–¹æ³•
        """
        # æ–¹æ³•1ï¼šç°¡åŒ–çš„ç‰¹å¾µåˆ†æï¼ˆç•¶å‰å¯¦ç¾ï¼‰
        feature_sum = np.sum(np.abs(embedding))
        normalized_score = (feature_sum % 1.0)  # Simple normalization
        
        # Add some randomness for demonstration
        noise = np.random.normal(0, 0.1)
        probability = max(0.0, min(1.0, normalized_score + noise))
        
        return probability
    
    def _calculate_phishing_probability_advanced(self, embedding: np.ndarray, 
                                               reference_embeddings: Dict[str, np.ndarray] = None) -> float:
        """
        æ›´é€²éšçš„é‡£é­šéƒµä»¶æ©Ÿç‡è¨ˆç®—æ–¹æ³•
        ä½¿ç”¨èˆ‡è¨“ç·´æ™‚ç›¸ä¼¼çš„ç›¸ä¼¼åº¦æ¯”è¼ƒé‚è¼¯
        """
        if reference_embeddings is None:
            # å›é€€åˆ°ç°¡åŒ–æ–¹æ³•
            return self._calculate_phishing_probability(embedding)
        
        try:
            from sklearn.metrics.pairwise import cosine_similarity
            
            # èˆ‡é‡£é­šéƒµä»¶åƒè€ƒå‘é‡çš„ç›¸ä¼¼åº¦
            phishing_similarities = []
            safe_similarities = []
            
            # è¨ˆç®—èˆ‡å·²çŸ¥é‡£é­šéƒµä»¶çš„ç›¸ä¼¼åº¦
            if 'phishing_examples' in reference_embeddings:
                phishing_refs = reference_embeddings['phishing_examples']
                for ref_emb in phishing_refs:
                    sim = cosine_similarity([embedding], [ref_emb])[0][0]
                    phishing_similarities.append(sim)
            
            # è¨ˆç®—èˆ‡å·²çŸ¥å®‰å…¨éƒµä»¶çš„ç›¸ä¼¼åº¦
            if 'safe_examples' in reference_embeddings:
                safe_refs = reference_embeddings['safe_examples']
                for ref_emb in safe_refs:
                    sim = cosine_similarity([embedding], [ref_emb])[0][0]
                    safe_similarities.append(sim)
            
            # åŸºæ–¼ç›¸ä¼¼åº¦è¨ˆç®—æ©Ÿç‡
            avg_phishing_sim = np.mean(phishing_similarities) if phishing_similarities else 0.0
            avg_safe_sim = np.mean(safe_similarities) if safe_similarities else 0.0
            
            # ç°¡å–®çš„æ©Ÿç‡è¨ˆç®—
            if avg_phishing_sim + avg_safe_sim > 0:
                probability = avg_phishing_sim / (avg_phishing_sim + avg_safe_sim)
            else:
                probability = 0.5  # ç„¡æ³•åˆ¤æ–·æ™‚è¿”å›ä¸­æ€§å€¼
            
            return max(0.0, min(1.0, probability))
            
        except Exception as e:
            logger.warning(f"Advanced probability calculation failed: {e}")
            return self._calculate_phishing_probability(embedding)
    
    def batch_inference(self, text_list: List[str], 
                       model_name: str = "default",
                       batch_size: int = 32) -> Dict[str, Any]:
        """Synchronous batch inference for multiple texts"""
        try:
            model = self.model_manager.load_model(model_name)
            
            # Process in batches
            all_predictions = []
            all_confidences = []
            
            for i in range(0, len(text_list), batch_size):
                batch_texts = text_list[i:i + batch_size]
                
                # Generate embeddings for this batch
                embeddings = model.encode(
                    batch_texts,
                    convert_to_numpy=True,
                    show_progress_bar=False
                )
                
                # Get predictions for this batch
                for embedding in embeddings:
                    confidence = self._calculate_phishing_probability(embedding)
                    prediction = 1 if confidence > 0.5 else 0
                    all_predictions.append(prediction)
                    all_confidences.append(confidence)
            
            return {
                'predictions': all_predictions,
                'confidences': all_confidences,
                'total_processed': len(text_list),
                'model_used': model_name
            }
            
        except Exception as e:
            logger.error(f"Batch inference failed: {e}")
            raise
    
    def get_model_predictions_summary(self, predictions: List[int], 
                                    confidences: List[float]) -> Dict[str, Any]:
        """Generate summary statistics for predictions"""
        if not predictions:
            return {}
        
        phishing_count = sum(predictions)
        safe_count = len(predictions) - phishing_count
        
        avg_confidence = np.mean(confidences)
        max_confidence = np.max(confidences)
        min_confidence = np.min(confidences)
        
        high_confidence_threshold = 0.8
        high_confidence_count = sum(1 for c in confidences if c > high_confidence_threshold)
        
        return {
            'total_emails': len(predictions),
            'phishing_detected': phishing_count,
            'safe_emails': safe_count,
            'phishing_percentage': (phishing_count / len(predictions)) * 100,
            'average_confidence': avg_confidence,
            'max_confidence': max_confidence,
            'min_confidence': min_confidence,
            'high_confidence_predictions': high_confidence_count,
            'high_confidence_percentage': (high_confidence_count / len(predictions)) * 100
        }

    async def process_inference_advanced(self, request: InferenceRequest, 
                                      method: str = "few_shot",
                                      reference_examples: Dict[str, List[str]] = None) -> Dict[str, Any]:
        """
        é€²éšæ¨ç†è™•ç†ï¼Œæ”¯æ´å¤šç¨®æ¨ç†æ–¹æ³•
        """
        try:
            model = self.model_manager.load_model(request.model_name)
            
            if method == "zero_shot":
                # çœŸæ­£çš„ Zero-Shot åˆ†é¡
                results = InferenceMethod.zero_shot_classification(request.text_data, model)
                
            elif method == "few_shot" and reference_examples:
                # Few-Shot åˆ†é¡ï¼ˆé¡ä¼¼æ‚¨çš„è¨“ç·´ä»£ç¢¼ï¼‰
                results = InferenceMethod.few_shot_classification(
                    request.text_data, model, reference_examples
                )
                
            elif method == "keyword":
                # åŸºæ–¼é—œéµè©çš„åˆ†é¡
                results = InferenceMethod.keyword_based_classification(request.text_data)
                
            else:
                # å›é€€åˆ°åŸå§‹æ–¹æ³•
                return await self.process_inference(request)
            
            # è½‰æ›çµæœæ ¼å¼
            predictions = []
            confidences = []
            
            for result in results:
                phishing_prob = result['phishing_probability']
                prediction = 1 if phishing_prob > request.threshold else 0
                predictions.append(prediction)
                confidences.append(phishing_prob)
            
            return {
                'predictions': predictions,
                'confidences': confidences,
                'text_data': request.text_data,
                'method_used': method,
                'threshold_used': request.threshold,
                'model_used': request.model_name,
                'detailed_results': results
            }
            
        except Exception as e:
            logger.error(f"Advanced inference failed: {e}")
            raise


# æ–°å¢ï¼šä¸åŒçš„æ¨ç†æ–¹æ³•
class InferenceMethod:
    """ä¸åŒæ¨ç†æ–¹æ³•çš„å¯¦ç¾"""
    
    @staticmethod
    def zero_shot_classification(text_list: List[str], model) -> List[Dict[str, float]]:
        """
        çœŸæ­£çš„ Zero-Shot åˆ†é¡
        ä½¿ç”¨æ¨¡å‹çš„èªç¾©ç†è§£èƒ½åŠ›
        """
        results = []
        
        for text in text_list:
            # æ§‹é€  zero-shot æç¤º
            phishing_prompt = f"This email is a phishing attempt: {text}"
            safe_prompt = f"This email is legitimate and safe: {text}"
            
            # è¨ˆç®—å…©ç¨®æƒ…æ³ä¸‹çš„åµŒå…¥ç›¸ä¼¼åº¦
            text_embedding = model.encode([text])[0]
            phishing_embedding = model.encode([phishing_prompt])[0]
            safe_embedding = model.encode([safe_prompt])[0]
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            try:
                from sklearn.metrics.pairwise import cosine_similarity
                phishing_sim = cosine_similarity([text_embedding], [phishing_embedding])[0][0]
                safe_sim = cosine_similarity([text_embedding], [safe_embedding])[0][0]
            except ImportError:
                # Fallback without sklearn
                phishing_sim = np.dot(text_embedding, phishing_embedding) / (
                    np.linalg.norm(text_embedding) * np.linalg.norm(phishing_embedding)
                )
                safe_sim = np.dot(text_embedding, safe_embedding) / (
                    np.linalg.norm(text_embedding) * np.linalg.norm(safe_embedding)
                )
            
            # åŸºæ–¼ç›¸ä¼¼åº¦è¨ˆç®—æ©Ÿç‡
            total_sim = phishing_sim + safe_sim
            if total_sim > 0:
                phishing_prob = phishing_sim / total_sim
            else:
                phishing_prob = 0.5
            
            results.append({
                'phishing_probability': phishing_prob,
                'safe_probability': 1 - phishing_prob,
                'phishing_similarity': phishing_sim,
                'safe_similarity': safe_sim
            })
        
        return results
    
    @staticmethod
    def few_shot_classification(text_list: List[str], model, 
                               reference_examples: Dict[str, List[str]]) -> List[Dict[str, float]]:
        """
        Few-Shot åˆ†é¡ï¼ˆé¡ä¼¼æ‚¨çš„è¨“ç·´ä»£ç¢¼é‚è¼¯ï¼‰
        ä½¿ç”¨åƒè€ƒç¯„ä¾‹é€²è¡Œç›¸ä¼¼åº¦æ¯”è¼ƒ
        """
        results = []
        
        # è¨ˆç®—åƒè€ƒç¯„ä¾‹çš„åµŒå…¥
        phishing_examples = reference_examples.get('phishing', [])
        safe_examples = reference_examples.get('safe', [])
        
        phishing_embeddings = model.encode(phishing_examples) if phishing_examples else []
        safe_embeddings = model.encode(safe_examples) if safe_examples else []
        
        for text in text_list:
            text_embedding = model.encode([text])[0]
            
            # è¨ˆç®—èˆ‡é‡£é­šéƒµä»¶ç¯„ä¾‹çš„ç›¸ä¼¼åº¦
            phishing_similarities = []
            if len(phishing_embeddings) > 0:
                try:
                    from sklearn.metrics.pairwise import cosine_similarity
                    similarities = cosine_similarity([text_embedding], phishing_embeddings)[0]
                    phishing_similarities = similarities.tolist()
                except ImportError:
                    # Fallback without sklearn
                    for emb in phishing_embeddings:
                        sim = np.dot(text_embedding, emb) / (
                            np.linalg.norm(text_embedding) * np.linalg.norm(emb)
                        )
                        phishing_similarities.append(sim)
            
            # è¨ˆç®—èˆ‡å®‰å…¨éƒµä»¶ç¯„ä¾‹çš„ç›¸ä¼¼åº¦
            safe_similarities = []
            if len(safe_embeddings) > 0:
                try:
                    from sklearn.metrics.pairwise import cosine_similarity
                    similarities = cosine_similarity([text_embedding], safe_embeddings)[0]
                    safe_similarities = similarities.tolist()
                except ImportError:
                    # Fallback without sklearn
                    for emb in safe_embeddings:
                        sim = np.dot(text_embedding, emb) / (
                            np.linalg.norm(text_embedding) * np.linalg.norm(emb)
                        )
                        safe_similarities.append(sim)
            
            # åŸºæ–¼æœ€é«˜ç›¸ä¼¼åº¦é€²è¡Œåˆ†é¡
            max_phishing_sim = max(phishing_similarities) if phishing_similarities else 0.0
            max_safe_sim = max(safe_similarities) if safe_similarities else 0.0
            
            # è¨ˆç®—æ©Ÿç‡
            if max_phishing_sim + max_safe_sim > 0:
                phishing_prob = max_phishing_sim / (max_phishing_sim + max_safe_sim)
            else:
                phishing_prob = 0.5
            
            results.append({
                'phishing_probability': phishing_prob,
                'safe_probability': 1 - phishing_prob,
                'max_phishing_similarity': max_phishing_sim,
                'max_safe_similarity': max_safe_sim
            })
        
        return results
    
    @staticmethod
    def keyword_based_classification(text_list: List[str]) -> List[Dict[str, float]]:
        """
        åŸºæ–¼é—œéµè©çš„å•Ÿç™¼å¼åˆ†é¡
        """
        phishing_keywords = [
            'urgent', 'verify', 'account', 'suspended', 'click', 'immediately',
            'winner', 'prize', 'congratulations', 'limited time', 'act now',
            'confirm', 'update', 'security alert', 'suspended'
        ]
        
        results = []
        
        for text in text_list:
            text_lower = text.lower()
            keyword_count = sum(1 for keyword in phishing_keywords if keyword in text_lower)
            
            # åŸºæ–¼é—œéµè©æ•¸é‡è¨ˆç®—æ©Ÿç‡
            max_keywords = len(phishing_keywords)
            phishing_prob = min(0.9, keyword_count / max_keywords * 2)  # æœ€é«˜90%
            
            results.append({
                'phishing_probability': phishing_prob,
                'safe_probability': 1 - phishing_prob,
                'keyword_matches': keyword_count
            })
        
        return results


def quick_inference(text_list: List[str], model_name: str = "default") -> Dict[str, Any]:
    """Quick inference function for standalone use"""
    try:
        from core.model_manager import ModelManager
        
        # Create temporary model manager and inference engine
        model_manager = ModelManager("./models")
        inference_engine = InferenceEngine(model_manager)
        
        # Perform batch inference
        result = inference_engine.batch_inference(text_list, model_name)
        return result
        
    except Exception as e:
        logger.error(f"Quick inference failed: {e}")
        # Return mock result for testing
        return {
            'predictions': [np.random.randint(0, 2) for _ in text_list],
            'confidences': [np.random.uniform(0.3, 0.9) for _ in text_list],
            'total_processed': len(text_list),
            'model_used': model_name,
            'error': str(e)
        }


# æ–°å¢ï¼šç°¡å–®çš„æ¸¬è©¦å‡½æ•¸
def test_inference_engine():
    """Test function for inference engine"""
    print("ğŸ§ª Testing Inference Engine...")
    
    # Sample email texts
    sample_emails = [
        "Urgent: Your account will be suspended unless you verify immediately!",
        "Hi team, please find the quarterly report attached for your review.",
        "WINNER! You've won $1,000,000! Click here to claim your prize now!"
    ]
    
    try:
        # Test quick inference
        result = quick_inference(sample_emails)
        
        print(f"âœ… Processed {result['total_processed']} emails")
        print(f"ğŸ“Š Results: {result['predictions']}")
        print(f"ğŸ“ˆ Confidences: {[f'{c:.2f}' for c in result['confidences']]}")
        
        # Test summary
        try:
            from core.model_manager import ModelManager
            model_manager = ModelManager("./models")
            inference_engine = InferenceEngine(model_manager)
            
            summary = inference_engine.get_model_predictions_summary(
                result['predictions'], 
                result['confidences']
            )
            
            print(f"ğŸ“‹ Summary:")
            print(f"  - Phishing detected: {summary['phishing_detected']}/{summary['total_emails']}")
            print(f"  - Average confidence: {summary['average_confidence']:.2f}")
            print(f"  - High confidence predictions: {summary['high_confidence_predictions']}")
        except Exception as e:
            print(f"âš ï¸ Summary test skipped: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


# åªåœ¨ç›´æ¥é‹è¡Œæ­¤æ–‡ä»¶æ™‚åŸ·è¡Œæ¸¬è©¦ï¼Œé¿å…åœ¨importæ™‚åŸ·è¡Œ
if __name__ == "__main__":
    # Run test when file is executed directly
    test_inference_engine()