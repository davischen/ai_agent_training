# -*- coding: utf-8 -*-
"""
ATLAS - Adaptive Training and Learning Automation System
Main AI Training Agent that orchestrates all components
"""

import os
import logging
import asyncio
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from threading import Thread
import schedule

from .model_manager import ModelManager
from .inference_engine import InferenceEngine, InferenceRequest
from .training_engine import TrainingEngine, TrainingRequest, DataGenerator
from .scheduler import TaskScheduler, TaskPriority, ResourceRequirement

logger = logging.getLogger(__name__)

class AITrainingAgent:
    """Main AI Training Agent that orchestrates all components"""
    
    def __init__(self, model_dir: str = "models", max_concurrent_tasks: int = 10):
        # Initialize core components
        self.model_manager = ModelManager(model_dir)
        self.data_generator = DataGenerator()
        self.inference_engine = InferenceEngine(self.model_manager)
        self.training_engine = TrainingEngine(self.model_manager, self.data_generator)
        self.scheduler = TaskScheduler(max_concurrent_tasks=max_concurrent_tasks)
        
        # Start scheduler
        self.scheduler.start()
        
        # Setup periodic tasks
        self._setup_periodic_tasks()
        
        logger.info("ATLAS AI Training Agent initialized successfully")
    
    def _setup_periodic_tasks(self):
        """Setup periodic maintenance and monitoring tasks"""
        # Schedule daily maintenance at 2:00 AM
        schedule.every().day.at("02:00").do(self._daily_maintenance)
        
        # Schedule hourly health checks
        schedule.every().hour.do(self._hourly_health_check)
        
        # Start schedule runner thread
        schedule_thread = Thread(target=self._run_schedule, daemon=True)
        schedule_thread.start()
    
    def _run_schedule(self):
        """Run periodic scheduled tasks"""
        while True:
            try:
                schedule.run_pending()
                import time
                time.sleep(60)  # Check every minute
            except Exception as e:
                logger.error(f"Error in schedule runner: {e}")
                import time
                time.sleep(300)  # Wait 5 minutes on error
    
    def _daily_maintenance(self):
        """Daily maintenance tasks"""
        logger.info("Running daily maintenance...")
        try:
            # Cleanup old completed tasks
            cutoff_time = datetime.now() - timedelta(days=7)
            old_tasks = [
                task_id for task_id, result in self.scheduler.completed_tasks.items()
                if hasattr(result, 'completed_at') and result.completed_at and result.completed_at < cutoff_time
            ]
            for task_id in old_tasks:
                del self.scheduler.completed_tasks[task_id]
            
            logger.info(f"Cleaned up {len(old_tasks)} old tasks")
            
            # Model performance check
            model_info = self.model_manager.list_models()
            logger.info(f"Current models: {list(model_info.keys())}")
            
        except Exception as e:
            logger.error(f"Error in daily maintenance: {e}")
    
    def _hourly_health_check(self):
        """Hourly system health check"""
        logger.info("Running health check...")
        try:
            # Check scheduler status
            queue_status = self.scheduler.get_queue_status()
            logger.info(f"Queue status: {queue_status}")
            
            # Check resource utilization
            if self.scheduler.resource_monitor:
                resources = self.scheduler.resource_monitor.get_resource_utilization()
                logger.info(f"Resource utilization: {resources}")
            
        except Exception as e:
            logger.error(f"Error in health check: {e}")
    
    async def submit_inference_request(self, request: InferenceRequest) -> str:
        """Submit an inference request"""
        task_id = self.scheduler.schedule_task(
            task_payload=request,
            task_type="inference",
            priority=TaskPriority.HIGH,
            resource_requirements=ResourceRequirement(
                cpu_cores=1.0,
                memory_mb=2048,
                estimated_duration_seconds=30
            )
        )
        logger.info(f"Submitted inference request: {task_id}")
        return task_id
    
    async def submit_training_request(self, request: TrainingRequest) -> str:
        """Submit a training request"""
        task_id = self.scheduler.schedule_task(
            task_payload=request,
            task_type="training",
            priority=TaskPriority.NORMAL,
            resource_requirements=ResourceRequirement(
                cpu_cores=4.0,
                memory_mb=8192,
                gpu_memory_mb=4096,
                estimated_duration_seconds=3600
            ),
            scheduled_at=request.scheduled_at
        )
        logger.info(f"Submitted training request: {task_id}")
        return task_id
    
    def get_task_status(self, task_id: str):
        """Get the status of a task"""
        return self.scheduler.get_task_status(task_id)
    
    def generate_inference_request(self, text_data: List[str], **kwargs) -> InferenceRequest:
        """Generate an inference request with default parameters"""
        return InferenceRequest(
            request_id=str(uuid.uuid4()),
            text_data=text_data,
            model_name=kwargs.get('model_name', 'all-MiniLM-L6-v2'),
            batch_size=kwargs.get('batch_size', 32),
            return_embeddings=kwargs.get('return_embeddings', False),
            threshold=kwargs.get('threshold', 0.5)
        )
    
    def generate_training_request(self, 
                                task_type: str = "training",
                                **kwargs) -> TrainingRequest:
        """Generate a training request with default parameters"""
        return TrainingRequest(
            request_id=str(uuid.uuid4()),
            task_type=task_type,
            priority=kwargs.get('priority', 1),
            data_path=kwargs.get('data_path'),
            model_config=kwargs.get('model_config', {'base_model': 'all-MiniLM-L6-v2'}),
            training_params=kwargs.get('training_params', {'epochs': 3, 'batch_size': 16}),
            scheduled_at=kwargs.get('scheduled_at')
        )
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive system status"""
        try:
            queue_status = self.scheduler.get_queue_status()
            model_info = self.model_manager.list_models()
            
            status = {
                'timestamp': datetime.now().isoformat(),
                'scheduler': queue_status,
                'models': model_info,
                'system': {
                    'max_concurrent_tasks': self.scheduler.max_concurrent_tasks,
                    'resource_management_enabled': self.scheduler.enable_resource_management
                }
            }
            
            if self.scheduler.resource_monitor:
                status['resources'] = self.scheduler.resource_monitor.get_resource_utilization()
            
            return status
            
        except Exception as e:
            logger.error(f"Error getting system status: {e}")
            return {'error': str(e)}
    
    def load_model(self, model_name: str):
        """Load a specific model"""
        return self.model_manager.load_model(model_name)
    
    def list_available_models(self) -> Dict[str, str]:
        """List all available models"""
        return self.model_manager.list_models()
    
    async def quick_inference(self, texts: List[str], 
                            model_name: str = "all-MiniLM-L6-v2") -> Dict[str, Any]:
        """Perform quick inference without going through scheduler"""
        try:
            request = self.generate_inference_request(
                text_data=texts,
                model_name=model_name
            )
            
            result = await self.inference_engine.process_inference(request)
            return result
            
        except Exception as e:
            logger.error(f"Quick inference failed: {e}")
            raise
    
    def create
